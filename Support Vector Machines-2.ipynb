{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines Assignments-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Polynomial functions are used to transform data into a higher-dimensional space to enable linear separation.\n",
    "# Polynomial kernel functions provide an extremely efficient way to achieve this transformation implicitly, \n",
    "# avoiding the computational burden of explicitly calculating the transformed features. They are a crucial part \n",
    "# of the kernel trick, which makes SVMs so powerful for non-linear classification problems.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate non-linearly separable dataset (moons dataset)\n",
    "X, y = make_moons(n_samples=500, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train svc with polynomial kernel\n",
    "svc_poly = SVC(kernel=\"poly\", degree=3, C = 1.0)\n",
    "svc_poly.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracyx\n",
    "accuracy = svc_poly.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# In Support Vector Regression (SVR), the parameter ε (epsilon) defines a margin of tolerance around the \n",
    "# predicted function. The model does not penalize errors that fall within this ε-tube.\n",
    "\n",
    "# Impact of Increasing ε:\n",
    "\n",
    "    # Fewer Support Vectors:\n",
    "    # As ε increases, the model allows more points to fall within the margin without contributing to the loss.\n",
    "    # This means fewer data points become support vectors, as only those outside the margin affect the optimization.\n",
    "\n",
    "    # Simpler Model, Higher Bias:\n",
    "    # A large ε leads to a simpler model because fewer support vectors define the regression function.\n",
    "    # However, the model may underfit since it ignores small deviations from the trend.\n",
    "\n",
    "    # Lower Training Complexity:\n",
    "    # Fewer support vectors result in faster computation, making the model more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "# affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "# and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Support Vector Regression (SVR) is highly sensitive to hyperparameters, which influence its flexibility,\n",
    "# accuracy, and generalization.\n",
    "\n",
    "# 1. Kernel Function (kernel)\n",
    "# The kernel function transforms the input space into a higher-dimensional space where a linear relationship \n",
    "# can be established. The choice of kernel significantly affects the model’s complexity and accuracy.\n",
    "\n",
    "# Common Kernels in SVR:\n",
    "# Linear Kernel (kernel='linear')\n",
    "# Used when data has a linear relationship.\n",
    "# Example: Predicting house prices based on square footage.\n",
    "\n",
    "# Polynomial Kernel (kernel='poly')\n",
    "# Used when the relationship between variables follows a polynomial function.\n",
    "# Example: Modeling quadratic trends in stock prices.\n",
    "\n",
    "# Radial Basis Function (RBF) Kernel (kernel='rbf')\n",
    "# Most commonly used, captures non-linear relationships.\n",
    "# Example: Temperature prediction based on multiple weather factors.\n",
    "\n",
    "# Sigmoid Kernel (kernel='sigmoid')\n",
    "# Used in specific cases like neural networks.\n",
    "# Example: Classification-like problems in regression.\n",
    "\n",
    "# When to change the kernel?\n",
    "# Use linear if the data is linearly separable.\n",
    "# Use rbf for complex, non-linear data.\n",
    "# Use poly if you suspect a polynomial relationship.\n",
    "\n",
    "# 2. C Parameter (Regularization Parameter)\n",
    "# The C parameter controls the tradeoff between achieving a low error and having a smooth model.\n",
    "\n",
    "# Effects of C:\n",
    "# Higher C (e.g., C=100):\n",
    "# Lower bias, higher variance (fits data tightly).\n",
    "# Model tries to minimize errors strictly.\n",
    "# Risk of overfitting.\n",
    "# Lower C (e.g., C=0.1):\n",
    "# Higher bias, lower variance (more tolerance for errors).\n",
    "# Model generalizes better but may underfit.\n",
    "\n",
    "# When to adjust C?\n",
    "\n",
    "# If the model is overfitting, decrease C.\n",
    "# If the model is underfitting, increase C.\n",
    "\n",
    "# 3. Epsilon (ε) Parameter (Margin of Tolerance)\n",
    "# The epsilon (ε) parameter defines a margin around the predicted function where errors are ignored. \n",
    "# The model only penalizes errors outside this margin.\n",
    "\n",
    "# Effects of ε:\n",
    "# Smaller ε (e.g., ε=0.01)\n",
    "# More sensitive to small deviations.\n",
    "# Captures finer details but risks overfitting.\n",
    "# Larger ε (e.g., ε=1.0)\n",
    "# More tolerant of small errors.\n",
    "# Leads to a simpler, smoother model.\n",
    "\n",
    "# When to adjust ε?\n",
    "# For precise predictions, use a small ε.\n",
    "# For a smooth function that ignores noise, use a large ε.\n",
    "\n",
    "# 4. Gamma (γ) Parameter (For RBF and Polynomial Kernels)\n",
    "# The gamma (γ) parameter controls how far the influence of a single training point extends.\n",
    "\n",
    "# Effects of γ:\n",
    "# Higher γ (e.g., γ=10)\n",
    "# Each point has a shorter influence → captures fine details.\n",
    "# Can overfit the data.\n",
    "# Lower γ (e.g., γ=0.1)\n",
    "# Points have a wider influence → results in a smoother decision function.\n",
    "# May underfit the data.\n",
    "\n",
    "# When to adjust γ?\n",
    "# If the model is too complex (overfitting), lower γ.\n",
    "# If the model is too simple (underfitting), increase γ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Assignment:\n",
    "# Import the necessary libraries and load the datase\n",
    "# Split the dataset into training and testing set\n",
    "# Preprocess the data using any technique of your choice (e.g. scaling, normalization\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "# Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "# precision, recall, F1-score\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
    "# improve its performance\n",
    "# Train the tuned classifier on the entire datase\n",
    "# Save the trained classifier to a file for future use.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size= 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create svc\n",
    "Svc = SVC(C=1.0)\n",
    "\n",
    "# Train the model\n",
    "Svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = Svc.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(f\"Model Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],   # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Kernel types\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Gamma values\n",
    "    'degree': [2, 3, 4]  # Polynomial degree (only for 'poly' kernel)\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svc = SVC()\n",
    "\n",
    "# GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Pickle the model (save to disk)\n",
    "filename = 'svc_model.pkl' \n",
    "with open(filename, 'wb') as file:  # 'wb' for write binary\n",
    "    pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
